{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MCWTfrI35gy",
        "outputId": "b0716d31-b57d-4acf-9978-e6a8febd4921"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPUBxG1l4Gv2",
        "outputId": "d79a2dc4-8278-4da0-c6cd-a81563bf3513"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/DL_Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DL_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrjUNUo0EYY-",
        "outputId": "68302dde-6c81-40fc-d421-fff10369c046"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 17661675809291784419, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15505193728\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 9277990931717346440\n",
              " physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qnWmC404kjW"
      },
      "source": [
        "import numpy as np\n",
        "# !pip install torch\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "batch_size = 64\n",
        "n_workers = 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnjOr2rRVMe"
      },
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        imgs = os.listdir(main_dir)\n",
        "        self.total_imgs = []\n",
        "        for i in imgs:\n",
        "          if 'frame' in i:\n",
        "            self.total_imgs.append(i) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),])\n",
        "\n",
        "dset = CustomDataSet(\"unique-142p\", transform)\n",
        "loader = DataLoader(dset, batch_size=batch_size, shuffle=True, num_workers=n_workers, pin_memory=False, drop_last=True)\n",
        "\n",
        "dset_0 = CustomDataSet(\"scene-change-examples/scene-0\", transform)\n",
        "loader_0 = DataLoader(dset, batch_size=batch_size, shuffle=False, num_workers=n_workers, pin_memory=False, drop_last=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk79MR_xaZCF"
      },
      "source": [
        "def reconstruction_loss(x, x_recon, distribution):\n",
        "    batch_size = x.size(0)\n",
        "    assert batch_size != 0\n",
        "\n",
        "    if distribution == 'bernoulli':\n",
        "        recon_loss = F.binary_cross_entropy_with_logits(x_recon, x, size_average=False).div(batch_size)\n",
        "    elif distribution == 'gaussian':\n",
        "        x_recon = F.sigmoid(x_recon)\n",
        "        recon_loss = F.mse_loss(x_recon, x, size_average=False).div(batch_size)\n",
        "    else:\n",
        "        recon_loss = None\n",
        "\n",
        "    return recon_loss\n",
        "\n",
        "\n",
        "def kl_divergence(mu, logvar):\n",
        "    batch_size = mu.size(0)\n",
        "    assert batch_size != 0\n",
        "    if mu.data.ndimension() == 4:\n",
        "        mu = mu.view(mu.size(0), mu.size(1))\n",
        "    if logvar.data.ndimension() == 4:\n",
        "        logvar = logvar.view(logvar.size(0), logvar.size(1))\n",
        "\n",
        "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    total_kld = klds.sum(1).mean(0, True)\n",
        "    dimension_wise_kld = klds.mean(0)\n",
        "    mean_kld = klds.mean(1).mean(0, True)\n",
        "\n",
        "    return total_kld, dimension_wise_kld, mean_kld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtVezpqA5AAY"
      },
      "source": [
        "def reparametrize(mu, logvar):\n",
        "    std = logvar.div(2).exp()\n",
        "    eps = Variable(std.data.new(std.size()).normal_())\n",
        "    return mu + std*eps\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(View, self).__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return tensor.view(self.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNJCTtF84RuH"
      },
      "source": [
        "class BetaVAE_H(nn.Module):\n",
        "    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017).\"\"\"\n",
        "\n",
        "    def __init__(self, z_dim=10, nc=3):\n",
        "        super(BetaVAE_H, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.nc = nc\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n",
        "            nn.ReLU(True),\n",
        "            View((-1, 256*1*1)),                 # B, 256\n",
        "            nn.Linear(256, z_dim*2),             # B, z_dim*2\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),               # B, 256\n",
        "            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n",
        "        )\n",
        "\n",
        "        self.weight_init()\n",
        "\n",
        "    def weight_init(self):\n",
        "        for block in self._modules:\n",
        "            for m in self._modules[block]:\n",
        "                kaiming_init(m)\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "        distributions = self.encoder(x)\n",
        "        mu = distributions[:, :self.z_dim]\n",
        "        logvar = distributions[:, self.z_dim:]\n",
        "        z = reparametrize(mu, logvar)\n",
        "        x_recon = self.decoder(z)\n",
        "        if not train:\n",
        "          return x_recon, z\n",
        "          \n",
        "        return x_recon, mu, logvar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au0jHju74t2s"
      },
      "source": [
        "def kaiming_init(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDovOUIZ5HhE"
      },
      "source": [
        "model = BetaVAE_H()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFKPGkG2Cd6i"
      },
      "source": [
        "def train(model, optimiser, dataloader, beta):\n",
        "  model.train()\n",
        "  # self.C_max = Variable(cuda(torch.FloatTensor([self.C_max]), self.use_cuda))\n",
        "  rl = 0\n",
        "  bl = 0\n",
        "  kl = 0\n",
        "  trl = []\n",
        "  tbl = []\n",
        "  tkl = []\n",
        "  for i, x in enumerate(dataloader):\n",
        "    x = x.cuda()\n",
        "    x_recon, mu, logvar = model(x)\n",
        "    recon_loss = reconstruction_loss(x, x_recon, 'gaussian')\n",
        "    total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
        "\n",
        "    beta_vae_loss = recon_loss + beta*total_kld\n",
        "    \n",
        "    optimiser.zero_grad()\n",
        "    beta_vae_loss.backward()\n",
        "    optimiser.step()\n",
        "    \n",
        "    rl+=recon_loss.item()\n",
        "    kl+=total_kld.item()\n",
        "    bl+=beta_vae_loss.item()\n",
        "\n",
        "    if i % 20 == 0:\n",
        "        print('[{}] recon_loss:{:.3f} total_kld:{:.3f} mean_kld:{:.3f} beta_vae_loss:{:.3f}'.format(\n",
        "            i, recon_loss.item(), total_kld.item(), mean_kld.item(), beta_vae_loss.item()))\n",
        "\n",
        "        # var = logvar.exp().mean(0).data\n",
        "        # var_str = ''\n",
        "        # for j, var_j in enumerate(var):\n",
        "        #     var_str += 'var{}:{:.4f} '.format(j+1, var_j)\n",
        "        # print(var_str)\n",
        "\n",
        "        trl.append(recon_loss.item())\n",
        "        tkl.append(total_kld.item())\n",
        "        tbl.append(beta_vae_loss.item())\n",
        "\n",
        "  return rl/len(dataloader), bl/len(dataloader), kl/len(dataloader), trl, tbl, tkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKkOiUIjHvyV"
      },
      "source": [
        "model = BetaVAE_H()\n",
        "model.cuda()\n",
        "\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.5)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode=\"min\", factor=0.3, patience=1, verbose=True)\n",
        "epochs = 100\n",
        "model_no = 2\n",
        "beta = 10\n",
        "\n",
        "TRL, TBL, TKL, RL, BL, KL = [], [], [], [], [], []\n",
        "\n",
        "for i in range(epochs):\n",
        "    if path.exists('model_'+ str(model_no) + '_' + str(i)):\n",
        "        print(\"Load model for epoch\", i)\n",
        "        checkpoint = torch.load('model_' + str(model_no) + '_' + str(i))\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "        # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        i = checkpoint['epoch']\n",
        "        TRL = checkpoint['trl']\n",
        "        TBL = checkpoint['tbl']\n",
        "        TKL = checkpoint['tkl']\n",
        "        RL = checkpoint['rl']\n",
        "        BL = checkpoint['bl']\n",
        "        KL = checkpoint['kl']\n",
        "        \n",
        "        continue\n",
        "\n",
        "    # break    \n",
        "    print(scheduler.get_last_lr())\n",
        "    \n",
        "    print(\"Training epoch\", i)\n",
        "    start = time.time()\n",
        "    rl, bl, kl, trl, tbl, tkl = train(model, optimiser, loader, beta)\n",
        "    print(\"RL:\", rl, \"BL:\", bl, \"KL:\", kl)\n",
        "    print(\"Train time:\", time.time()-start)\n",
        "    \n",
        "    TRL += trl\n",
        "    TBL += tbl\n",
        "    TKL += tkl\n",
        "    RL.append(rl)\n",
        "    BL.append(bl)\n",
        "    KL.append(kl)\n",
        "    \n",
        "    # scheduler.step()\n",
        "    \n",
        "    torch.save({\n",
        "                'epoch': i,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimiser_state_dict': optimiser.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'trl': TRL,\n",
        "                'tbl': TBL,\n",
        "                'tkl': TKL,\n",
        "                'rl': RL,\n",
        "                'bl': BL,\n",
        "                'kl': KL,\n",
        "                }, 'model_' + str(model_no) + '_' + str(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igpe4FfiYWNa"
      },
      "source": [
        "\n",
        "# Model 0 - Beta = 250, gaussian\n",
        "# Model 1 - Beta = 4, gaussian\n",
        "# Model 2 - Beta = 10, gaussian\n",
        "# Model 4 - Beta = 1, gaussian\n",
        "# Model 3 - Beta = 250, bernoulli"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}